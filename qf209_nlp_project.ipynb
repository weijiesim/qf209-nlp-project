{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_mdkZqU2Zhnq",
        "outputId": "4ce0b414-8d9d-4f4d-8fd2-932158169f7f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import yfinance as yf\n",
        "import plotly.express as px\n",
        "import scipy\n",
        "from dash_bootstrap_templates import load_figure_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "load_figure_template('minty')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1 = pd.read_csv(\"data/daily_financial_news/analyst_ratings_processed.csv\", index_col=0)\n",
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1['date'] = data_1['date'].str.split(' ', expand=True).iloc[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1['stock'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVlMohoBlTJQ",
        "outputId": "15579c96-a538-480e-cf36-81553f02c734"
      },
      "outputs": [],
      "source": [
        "data_1.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drop Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKt6rDWBlk8l",
        "outputId": "9512fd8d-8077-4237-937c-e87a35976c89"
      },
      "outputs": [],
      "source": [
        "print(data_1.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9pDiHR3nCvC",
        "outputId": "606a9af2-c319-4559-b501-5f7cdd954638"
      },
      "outputs": [],
      "source": [
        "print(data_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1[data_1['date'].isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV5Xn1-2oMTG"
      },
      "outputs": [],
      "source": [
        "data_1.dropna(subset=['date'],inplace=True)\n",
        "data_1.dropna(subset=['stock'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkszMKzlpVnL",
        "outputId": "f2b99e34-dab9-4f73-805c-c1db92b21862"
      },
      "outputs": [],
      "source": [
        "print(data_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG6ydygqpZx1",
        "outputId": "9fff9776-66d4-42ad-9307-309dd0fbd9f6"
      },
      "outputs": [],
      "source": [
        "print(data_1.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lthUY-R-phFG",
        "outputId": "39c496c4-1489-4457-93e0-5e88c8054b29"
      },
      "outputs": [],
      "source": [
        "data_1.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filter stocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter dataset down to stocks with top 100 number of headlines\n",
        "top_100_stocks_by_headlines = data_1.groupby('stock').size().reset_index(name='size').sort_values('size', ascending=False).reset_index(drop=True).iloc[:100]\n",
        "top_100_stocks_by_headlines.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1 = data_1[data_1['stock'].isin(top_100_stocks_by_headlines.stock)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bysKZg9qtnrg"
      },
      "outputs": [],
      "source": [
        "data_1['title'] = data_1['title'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "j0Ci37Nwt9fX",
        "outputId": "027046b6-7432-4457-8727-aebebc71e119"
      },
      "outputs": [],
      "source": [
        "data_1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jUv6dT5vpIp"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
        "\n",
        "data_1['title'] = data_1['title'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hHyB2atzvvgq",
        "outputId": "9c27a854-4218-4cd1-9f3f-c40293e8d38a"
      },
      "outputs": [],
      "source": [
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GdlIDQsTwJxt",
        "outputId": "3c5dfcd6-b1f5-4462-92e0-24d99fcaf567"
      },
      "outputs": [],
      "source": [
        "data_1['tokens'] = data_1['title'].apply(lambda x: x.split())\n",
        "data_1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUEjDSAwwSjK",
        "outputId": "4b5df763-3a4c-42db-ad0a-ff33b2c5c7aa"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "data_1['tokens'] = data_1['tokens'].apply(lambda x: [word for word in x if word not in stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mkPxf-CgwozT",
        "outputId": "2b6e2cf8-d361-48eb-ae5b-70632f6547dd"
      },
      "outputs": [],
      "source": [
        "data_1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lemmatize tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7fTt-K3xJvz",
        "outputId": "5c7566e1-e106-4449-ea98-8f15cb9a1c93"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "data_1['tokens'] = data_1['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "15KCoy4qxkxx",
        "outputId": "dc3de5f1-a6b2-4628-ac4d-a94ac0f98b39"
      },
      "outputs": [],
      "source": [
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XUau0f0GxULI",
        "outputId": "2bc02e85-0e41-43ce-bc32-64d619832a29"
      },
      "outputs": [],
      "source": [
        "data_1['preprocessed_text'] = data_1['tokens'].apply(' '.join)\n",
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDMEu5K2x4L6",
        "outputId": "20fc393c-bef0-41bf-ce79-d63044de3c74"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "# NLTK Sentiment Intensity Analyzer uses a 'Bag of Words' approach\n",
        "# it removes stop words and scores each word individually before compounding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_duplicate = data_1.copy()\n",
        "data_duplicate['sentiment_score'] = data_duplicate['preprocessed_text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "data_duplicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0nlV-P20K_8",
        "outputId": "2c43f90d-c726-46e3-c5d3-999274766fdf"
      },
      "outputs": [],
      "source": [
        "data_duplicate['sentiment_score'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGBvIQCZ2Ew6",
        "outputId": "ce24972b-4612-4b4d-86f8-32e1d288a9c0"
      },
      "outputs": [],
      "source": [
        "data_duplicate.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_duplicate.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "signals_df = data_duplicate[['date', 'stock', 'sentiment_score']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "signals_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# There are some stocks that have multiple news articles on the same day\n",
        "# Have to handle these cases\n",
        "non_dup_signals_df = signals_df.groupby(['date', 'stock'])['sentiment_score'].mean().reset_index(name='sentiment_score')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pull yfinance data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tickers = data_duplicate.stock.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_date, end_date = data_duplicate.date.sort_values().iloc[0], data_duplicate.date.sort_values().iloc[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = yf.download(list(tickers), start=start_date, end=end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adj_close_data = data['Adj Close']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tickers that don't have data\n",
        "missing_data_tickers = adj_close_data.columns[adj_close_data.isna().sum()/adj_close_data.shape[0] == 1]\n",
        "\n",
        "# Drop missing tickers\n",
        "adj_close_data = adj_close_data.drop(columns=missing_data_tickers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop other tickers\n",
        "adj_close_data = adj_close_data.dropna(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "any(adj_close_data.isna().sum() > 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "returns_df = adj_close_data.pct_change().dropna().reset_index().rename(columns={'Date': 'date'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "returns_df_melt = returns_df.melt(id_vars='date', var_name='stock', value_name='daily_returns')\n",
        "returns_df_melt['date'] = pd.to_datetime(returns_df_melt['date'])\n",
        "returns_df_melt.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_dup_signals_df.sort_values('date').head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "returns_df_melt.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_dup_signals_df['date'] = pd.to_datetime(non_dup_signals_df['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df = pd.merge(returns_df_melt, non_dup_signals_df, on=['date', 'stock'], how='left').dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count number of stocks per date\n",
        "merged_df['num_stocks_by_date'] = merged_df.groupby('date').transform('size')\n",
        "\n",
        "# Select data where there were at least 10 stocks for each date\n",
        "merged_df_filtered = merged_df[merged_df['num_stocks_by_date'] >= 10]\n",
        "\n",
        "# Drop num_stocks_by_date column\n",
        "ml_df = merged_df.drop(columns='num_stocks_by_date').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build Machine Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split Data into Features and Target\n",
        "# Sort values by date\n",
        "X = ml_df.sort_values(\"date\").drop('daily_returns', axis=1).reset_index(drop=True)\n",
        "y = ml_df.sort_values(\"date\")['daily_returns'].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ts_train_test_split(data, test_size):\n",
        "    \"\"\"Takes in data and output train set and test set in that order\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame or pd.Series): Data to split into train and test\n",
        "        test_size (float): Percentage for test size\n",
        "\n",
        "    Returns:\n",
        "        tuple: train set, test set\n",
        "    \"\"\"\n",
        "    train_size = 1-test_size\n",
        "    train_idx = round(X.shape[0] * train_size)\n",
        "    return data.iloc[:train_idx], data.iloc[train_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split Data into Training and Validation Sets\n",
        "X_train, X_valid = ts_train_test_split(X, test_size=0.2)\n",
        "y_train, y_valid = ts_train_test_split(y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "# One-hot Encode 'stock' Column for Training and Validation Data\n",
        "OH_cols_train = pd.DataFrame(one_hot_encoder.fit_transform(X_train[['stock']]))\n",
        "OH_cols_valid = pd.DataFrame(one_hot_encoder.transform(X_valid[['stock']]))\n",
        "\n",
        "# Assign Column Names after One-Hot Encoding and Restore Index\n",
        "OH_cols_train.columns = one_hot_encoder.get_feature_names_out(['stock'])\n",
        "OH_cols_valid.columns = one_hot_encoder.get_feature_names_out(['stock'])\n",
        "OH_cols_train.index, OH_cols_valid.index = X_train.index, X_valid.index\n",
        "\n",
        "# Remove Original 'stock' Column\n",
        "num_X_train = X_train.drop('stock', axis=1)\n",
        "num_X_valid = X_valid.drop('stock', axis=1)\n",
        "\n",
        "# Concatenate Original Data with One-Hot Encoded Columns\n",
        "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
        "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
        "\n",
        "# Ensure all columns have string type\n",
        "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
        "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
        "\n",
        "# Ensure 'date' is in datetime format if it is used in further analyses\n",
        "OH_X_train = OH_X_train.drop(columns=['date'])\n",
        "OH_X_valid = OH_X_valid.drop(columns=['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score_model(model, X_t, X_v, y_t, y_v):\n",
        "    # Fit Model\n",
        "    model.fit(X_t, y_t)\n",
        "\n",
        "    # Predict\n",
        "    preds = model.predict(X_v)\n",
        "\n",
        "    # Check MAE\n",
        "    mae = mean_absolute_error(preds, y_v)\n",
        "    return mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linear_model = LinearRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_model(linear_model, OH_X_train, OH_X_valid, y_train, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Portfolio Analytics Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_daily_ptf_rtn(ptf_wgt, returns_df, ls=False):\n",
        "    \"\"\"Calculate daily portfolio returns given long form portfolio weights and returns\n",
        "\n",
        "    Args:\n",
        "        ptf_wgt (pd.DataFrame): Long form id level portfolio weights\n",
        "        returns_df (pd.DataFrame): Long form id level returns\n",
        "    \"\"\"\n",
        "    ptf_wgt['DATE'] = pd.to_datetime(ptf_wgt['DATE'])\n",
        "    returns_df['DATE'] = pd.to_datetime(returns_df['DATE'])\n",
        "\n",
        "    start_date = ptf_wgt['DATE'].min()\n",
        "    end_date = ptf_wgt['DATE'].max()\n",
        "\n",
        "    filtered_returns = returns_df[(returns_df['DATE'] <= end_date) & (returns_df['DATE'] >= start_date)].reset_index(drop=True)\n",
        "    rebal_dates = ptf_wgt['DATE'].unique()\n",
        "\n",
        "    # Add rebal date column to returns df\n",
        "    filtered_returns['REBAL_DATE'] = filtered_returns['DATE'].apply(lambda x: rebal_dates[rebal_dates <= x].max())\n",
        "    joint_df = pd.merge(ptf_wgt.rename(columns={'DATE': 'REBAL_DATE'}), filtered_returns, on=['REBAL_DATE', 'ID'], how='left')\n",
        "\n",
        "    # Dates with no returns are filled as 0\n",
        "    joint_df = joint_df.fillna(0)\n",
        "\n",
        "    final_df = joint_df[['DATE', 'REBAL_DATE', 'ID', \"WGT\", \"RTN\"]].copy()\n",
        "\n",
        "    # Cumulate asset returns within each rebal date\n",
        "    final_df['ASSET_CUM_RTN'] = final_df.groupby(['REBAL_DATE', 'ID'])['RTN'].transform(lambda x: (1+x).cumprod())\n",
        "\n",
        "    # Calculate asset's MTM weight\n",
        "    final_df['MTM_WGT'] = final_df['WGT'] * final_df['ASSET_CUM_RTN']\n",
        "    final_df['DATE'] = pd.to_datetime(final_df['DATE'])\n",
        "\n",
        "    if ls:\n",
        "        final_df['LONG/SHORT'] = np.where(final_df['MTM_WGT'] > 0, 'LONG', 'SHORT')\n",
        "        final_df['PTF_MTM_BASE'] = final_df.groupby([\"DATE\", 'LONG/SHORT'])['MTM_WGT'].transform(lambda x: abs(x).sum())\n",
        "        final_df['ASSET_WEIGHTS'] = final_df['MTM_WGT'] / final_df['PTF_MTM_BASE']\n",
        "\n",
        "    else:\n",
        "        # Calculate portfolio MTM base weight\n",
        "        final_df['PTF_MTM_BASE'] = final_df.groupby(\"DATE\")['MTM_WGT'].transform('sum')\n",
        "\n",
        "        # Calculate renormed asset weights\n",
        "        final_df['ASSET_WEIGHTS'] = final_df['MTM_WGT'] / final_df['PTF_MTM_BASE']\n",
        "\n",
        "    # Shift asset weights down by 1 to represent implied lag of 1 day\n",
        "    final_df['ASSET_WEIGHTS_SHIFTED'] = final_df.groupby('ID')['ASSET_WEIGHTS'].shift(1)\n",
        "\n",
        "    # Drop NaNs introduced from shifting\n",
        "    final_df = final_df.dropna(axis=0)\n",
        "\n",
        "    ptf_rtn_df = final_df.groupby('DATE').apply(lambda x: (x['RTN'] * x['ASSET_WEIGHTS_SHIFTED']).sum()).reset_index(name=\"PTF_RTN\")\n",
        "    return ptf_rtn_df, final_df\n",
        "\n",
        "def calc_annualised_returns(cumulative_returns:float, n, frequency):\n",
        "    if frequency == \"D\":\n",
        "        t = 252\n",
        "    elif frequency == \"M\":\n",
        "        t = 12\n",
        "    return ((cumulative_returns + 1)**(t/n) - 1).values[0]\n",
        "\n",
        "def calc_annualised_vol(ptf_rtn: pd.Series, frequency):\n",
        "    if frequency == \"D\":\n",
        "        n = 365 # 365 trading days in ptf_rtn\n",
        "    elif frequency == \"M\":\n",
        "        n = 12\n",
        "    return ptf_rtn.std(ddof=1).values[0] * np.sqrt(n)\n",
        "\n",
        "def calc_max_dd(ptf_rtn: pd.Series):\n",
        "    # Cumulative returns must be base 1\n",
        "    ptf_cumulative_return = (1+ptf_rtn).cumprod()\n",
        "\n",
        "    # Calculate running max\n",
        "    running_max = ptf_cumulative_return.cummax()\n",
        "\n",
        "    # Drawdown\n",
        "    drawdown = (ptf_cumulative_return-running_max)/running_max\n",
        "\n",
        "    max_drawdown = drawdown.min().values[0]\n",
        "    return max_drawdown\n",
        "\n",
        "def calc_ptf_summary(ptf_rtn):\n",
        "    ptf_cum_rtn = (ptf_rtn+1).prod()-1\n",
        "    ptf_ann_rtn = calc_annualised_returns(ptf_cum_rtn, len(ptf_rtn), 'D')\n",
        "    ptf_ann_vol = calc_annualised_vol(ptf_rtn, \"D\")\n",
        "    ptf_max_dd = calc_max_dd(ptf_rtn)\n",
        "    sharpe_ratio = ptf_ann_rtn/ptf_ann_vol\n",
        "    downside_sd = ptf_rtn[ptf_rtn < 0].std()[0]\n",
        "    sortino_ratio = ptf_ann_rtn/downside_sd\n",
        "    return pd.DataFrame({\n",
        "        'Metrics': ['Cumulative Returns', 'Annualised Returns',\n",
        "                    'Annualised Volatility', 'Maximum Drawdown',\n",
        "                    'Sharpe Ratio', 'Sortino Ratio'],\n",
        "        'Values': [ptf_cum_rtn[0], ptf_ann_rtn, ptf_ann_vol, ptf_max_dd,\n",
        "                   sharpe_ratio, sortino_ratio]\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Backtest():\n",
        "    def __init__(self, model, rtn_df, OH_X_train, y_train, OH_X_valid, X_valid):\n",
        "        self.model = model\n",
        "        self.rtn_df = rtn_df\n",
        "        self.OH_X_train = OH_X_train\n",
        "        self.OH_X_valid = OH_X_valid\n",
        "        self.y_train = y_train\n",
        "        self.X_valid = X_valid\n",
        "\n",
        "        # Generate signal dataframe\n",
        "        self.sig_df =  self.gen_signals_df()\n",
        "\n",
        "        # Generate long short portfolio\n",
        "        self.ls_ptf_wgt = self.constr_ls_ptf_wgt()\n",
        "\n",
        "        # Generate backtest analytics\n",
        "        self.cum_rtn_fig, self.summary_metrics = self.gen_backtest_analytics()\n",
        "\n",
        "    def gen_signals_df(self):\n",
        "        self.model.fit(self.OH_X_train, self.y_train)\n",
        "        preds = self.model.predict(self.OH_X_valid)\n",
        "        trading_df = self.X_valid.copy()\n",
        "\n",
        "        # Create column of predicted returns\n",
        "        trading_df['predicted_rtn'] = preds\n",
        "\n",
        "        # Count number of stocks per date\n",
        "        trading_df['num_stocks_by_date'] = trading_df.groupby('date').transform('size')\n",
        "\n",
        "        # Select data where there were at least 10 stocks for each date\n",
        "        trading_df_filtered = trading_df[trading_df['num_stocks_by_date'] >= 10]\n",
        "\n",
        "        trading_df_filtered = trading_df_filtered[['date', 'stock', 'predicted_rtn']].reset_index(drop=True)\n",
        "\n",
        "        # Create daily index and forward fill\n",
        "        start_date = trading_df_filtered['date'].min()\n",
        "        end_date = trading_df_filtered['date'].max()\n",
        "\n",
        "        # Reesample to daily\n",
        "        daily_index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "        trading_df_filtered_wide = trading_df_filtered.pivot(index='date', values='predicted_rtn', columns='stock')\n",
        "        trading_df_filtered_wide = trading_df_filtered_wide.reindex(daily_index).fillna(method='ffill')\n",
        "\n",
        "        final_signal_df = (trading_df_filtered_wide\n",
        "                        .reset_index(names='date')\n",
        "                        .rename_axis(None, axis=1)\n",
        "                        .melt(id_vars='date', var_name='stock', value_name='signal')\n",
        "                        .dropna(axis=0).reset_index(drop=True))\n",
        "        final_signal_df = final_signal_df.rename(columns={\n",
        "            'date': 'DATE',\n",
        "            'stock': 'ID',\n",
        "            'signal': 'SIGNAL'\n",
        "        })\n",
        "        final_signal_df['RANKED_SIGNAL'] = final_signal_df.groupby('DATE')['SIGNAL'].transform(lambda x: scipy.stats.rankdata(x))\n",
        "        final_signal_df = final_signal_df.sort_values(['DATE', 'ID']).reset_index(drop=True)\n",
        "        return final_signal_df\n",
        "\n",
        "    def constr_ls_ptf_wgt(self):\n",
        "        # Long short weights calculated as the distance for median signal for each date\n",
        "        self.sig_df['WGT'] = self.sig_df.groupby(['DATE'])['RANKED_SIGNAL'].transform(lambda x: x-x.median())\n",
        "\n",
        "        # Renormalise weights to $1 long $1 short – dollar neutral strategy\n",
        "        self.sig_df['DIRECTION'] = np.where(self.sig_df['WGT']>=0, 'LONG', 'SHORT')\n",
        "        self.sig_df['RENORM_WGT'] = self.sig_df.groupby(['DATE', 'DIRECTION'])['WGT'].transform(lambda x: x/np.abs(x.sum()))\n",
        "        return self.sig_df[['DATE', 'ID', 'RENORM_WGT']].rename(columns={'RENORM_WGT': 'WGT'})\n",
        "\n",
        "    def gen_backtest_analytics(self):\n",
        "        # Merge weights on returns – left join\n",
        "        ptf_df = pd.merge(self.ls_ptf_wgt, self.rtn_df, on=['DATE', 'ID'], how='left')\n",
        "\n",
        "        # On dates without returns just set returns to 0\n",
        "        ptf_df = ptf_df.fillna(0)\n",
        "\n",
        "        # Shift weights by 1 to imply lag\n",
        "        ptf_df['LS_WGT_SHIFTED'] = ptf_df['WGT'].shift(1)\n",
        "\n",
        "        # Calculate long short weighted returns\n",
        "        ptf_df['LS_WGT_RTN'] = ptf_df['LS_WGT_SHIFTED'] * ptf_df['RTN']\n",
        "\n",
        "        # Derive equal weighted portfolio weights\n",
        "        ptf_df['EQ_WGT'] = ptf_df.groupby('DATE')['ID'].transform(lambda x: 1/x.shape[0])\n",
        "\n",
        "        # Shift EQ_WGT\n",
        "        ptf_df['EQ_WGT_SHIFTED'] = ptf_df['EQ_WGT'].shift(1)\n",
        "        ptf_df['EQ_WGT_RTN'] = ptf_df['EQ_WGT_SHIFTED'] * ptf_df['RTN']\n",
        "\n",
        "        # Drop NaN values from shifting\n",
        "        ptf_df = ptf_df.dropna()\n",
        "\n",
        "        # Melt to long form\n",
        "        ptf_df_port = (ptf_df.melt(id_vars='DATE',\n",
        "                                   value_vars=['LS_WGT_RTN', 'EQ_WGT_RTN'],\n",
        "                                   var_name='PORT', value_name='PTF_RTN'))\n",
        "\n",
        "        # Group by portfolio and calculate portfolio returns\n",
        "        ptf_rtn_combined = ptf_df_port.groupby(['DATE', 'PORT'])['PTF_RTN'].sum().reset_index()\n",
        "\n",
        "        # Cumulate portfolio returns\n",
        "        ptf_rtn_combined['CUM_RTN'] = ptf_rtn_combined.groupby(['PORT'])['PTF_RTN'].transform(lambda x: (1+x).cumprod())\n",
        "        ptf_rtn_combined\n",
        "\n",
        "        # Rename\n",
        "        ptf_rtn_combined['PORT'] = (ptf_rtn_combined['PORT']\n",
        "                                    .replace({'EQ_WGT_RTN': 'EQ_WGT_PORT',\n",
        "                                              'LS_WGT_RTN': 'LS_WGT_PORT'}))\n",
        "\n",
        "        # Generate cumulative returns figure\n",
        "        fig = px.line(ptf_rtn_combined, x='DATE', y='CUM_RTN', color='PORT')\n",
        "        fig.update_layout(hovermode='x unified')\n",
        "\n",
        "        # Create summary metrics table\n",
        "        ls_ptf_summary = calc_ptf_summary(ptf_rtn_combined.loc[ptf_rtn_combined['PORT']=='LS_WGT_PORT', ['DATE', 'PTF_RTN']].set_index('DATE'))\n",
        "        eq_ptf_summary = calc_ptf_summary(ptf_rtn_combined.loc[ptf_rtn_combined['PORT']=='EQ_WGT_PORT', ['DATE', 'PTF_RTN']].set_index('DATE'))\n",
        "        summary_metrics = pd.concat([ls_ptf_summary.set_index('Metrics'), eq_ptf_summary.set_index('Metrics')], axis=1)\n",
        "        summary_metrics.columns = ['Signals-Weighted Long Short Portfolio', 'Equal Weight Portfolio']\n",
        "        return fig, summary_metrics\n",
        "\n",
        "    def display_results(self):\n",
        "        display(self.summary_metrics)\n",
        "        self.cum_rtn_fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Long Short Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rtn_df = returns_df_melt.rename(columns={'date': 'DATE', 'stock': 'ID', 'daily_returns': 'RTN'})\n",
        "rtn_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linear_model_backtest = Backtest(linear_model, rtn_df, OH_X_train, y_train, OH_X_valid, X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linear_model_backtest.display_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Long only Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MPT Optimised Strategy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
