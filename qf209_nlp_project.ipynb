{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_mdkZqU2Zhnq",
        "outputId": "4ce0b414-8d9d-4f4d-8fd2-932158169f7f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import yfinance as yf\n",
        "import plotly.express as px\n",
        "import scipy\n",
        "from dash_bootstrap_templates import load_figure_template\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import xgboost as xgb\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from gensim import corpora, models\n",
        "import ast\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "load_figure_template('minty')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_pkl_file(object, filepath):\n",
        "    # Retrieve directory\n",
        "    directory = os.path.dirname(filepath)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Folder created at {directory}\")\n",
        "    with open(filepath, 'wb') as file:\n",
        "        pickle.dump(object, file)\n",
        "\n",
        "def load_pkl_file(filepath):\n",
        "    with open(filepath, 'rb') as file:\n",
        "        return pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1 = pd.read_csv(\"data/daily_financial_news/analyst_ratings_processed.csv\", index_col=0)\n",
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1['date'] = data_1['date'].str.split(' ', expand=True).iloc[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1['stock'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVlMohoBlTJQ",
        "outputId": "15579c96-a538-480e-cf36-81553f02c734"
      },
      "outputs": [],
      "source": [
        "data_1.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drop Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKt6rDWBlk8l",
        "outputId": "9512fd8d-8077-4237-937c-e87a35976c89"
      },
      "outputs": [],
      "source": [
        "print(data_1.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9pDiHR3nCvC",
        "outputId": "606a9af2-c319-4559-b501-5f7cdd954638"
      },
      "outputs": [],
      "source": [
        "print(data_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1[data_1['date'].isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV5Xn1-2oMTG"
      },
      "outputs": [],
      "source": [
        "data_1.dropna(subset=['date'],inplace=True)\n",
        "data_1.dropna(subset=['stock'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkszMKzlpVnL",
        "outputId": "f2b99e34-dab9-4f73-805c-c1db92b21862"
      },
      "outputs": [],
      "source": [
        "print(data_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG6ydygqpZx1",
        "outputId": "9fff9776-66d4-42ad-9307-309dd0fbd9f6"
      },
      "outputs": [],
      "source": [
        "print(data_1.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lthUY-R-phFG",
        "outputId": "39c496c4-1489-4457-93e0-5e88c8054b29"
      },
      "outputs": [],
      "source": [
        "data_1.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filter stocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter dataset down to stocks with top 100 number of headlines\n",
        "top_100_stocks_by_headlines = data_1.groupby('stock').size().reset_index(name='size').sort_values('size', ascending=False).reset_index(drop=True).iloc[:100]\n",
        "top_100_stocks_by_headlines.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1 = data_1[data_1['stock'].isin(top_100_stocks_by_headlines.stock)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bysKZg9qtnrg"
      },
      "outputs": [],
      "source": [
        "data_1['title'] = data_1['title'].str.lower()\n",
        "data_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jUv6dT5vpIp"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
        "\n",
        "data_1['title'] = data_1['title'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hHyB2atzvvgq",
        "outputId": "9c27a854-4218-4cd1-9f3f-c40293e8d38a"
      },
      "outputs": [],
      "source": [
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GdlIDQsTwJxt",
        "outputId": "3c5dfcd6-b1f5-4462-92e0-24d99fcaf567"
      },
      "outputs": [],
      "source": [
        "data_1['tokens'] = data_1['title'].apply(lambda x: x.split())\n",
        "data_1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUEjDSAwwSjK",
        "outputId": "4b5df763-3a4c-42db-ad0a-ff33b2c5c7aa"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "data_1['tokens'] = data_1['tokens'].apply(lambda x: [word for word in x if word not in stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mkPxf-CgwozT",
        "outputId": "2b6e2cf8-d361-48eb-ae5b-70632f6547dd"
      },
      "outputs": [],
      "source": [
        "data_1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lemmatize tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7fTt-K3xJvz",
        "outputId": "5c7566e1-e106-4449-ea98-8f15cb9a1c93"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "data_1['tokens'] = data_1['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "15KCoy4qxkxx",
        "outputId": "dc3de5f1-a6b2-4628-ac4d-a94ac0f98b39"
      },
      "outputs": [],
      "source": [
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XUau0f0GxULI",
        "outputId": "2bc02e85-0e41-43ce-bc32-64d619832a29"
      },
      "outputs": [],
      "source": [
        "data_1['preprocessed_text'] = data_1['tokens'].apply(' '.join)\n",
        "data_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDMEu5K2x4L6",
        "outputId": "20fc393c-bef0-41bf-ce79-d63044de3c74"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "# NLTK Sentiment Intensity Analyzer uses a 'Bag of Words' approach\n",
        "# it removes stop words and scores each word individually before compounding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_duplicate = data_1.copy()\n",
        "data_duplicate['sentiment_score'] = data_duplicate['preprocessed_text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "data_duplicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0nlV-P20K_8",
        "outputId": "2c43f90d-c726-46e3-c5d3-999274766fdf"
      },
      "outputs": [],
      "source": [
        "data_duplicate['sentiment_score'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGBvIQCZ2Ew6",
        "outputId": "ce24972b-4612-4b4d-86f8-32e1d288a9c0"
      },
      "outputs": [],
      "source": [
        "data_duplicate.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "signals_df = data_duplicate.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "signals_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# There are some stocks that have multiple news articles on the same day\n",
        "# Have to handle these cases\n",
        "# non_dup_signals_df = signals_df.groupby(['date', 'stock'])['sentiment_score'].mean().reset_index(name='sentiment_score')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Pull yfinance Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tickers = data_duplicate.stock.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_date, end_date = data_duplicate.date.sort_values().iloc[0], data_duplicate.date.sort_values().iloc[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = yf.download(list(tickers), start=start_date, end=end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adj_close_data = data['Adj Close']\n",
        "adj_close_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tickers that don't have data\n",
        "missing_data_tickers = adj_close_data.columns[adj_close_data.isna().sum()/adj_close_data.shape[0] == 1]\n",
        "\n",
        "# Drop missing tickers\n",
        "adj_close_data = adj_close_data.drop(columns=missing_data_tickers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop other tickers\n",
        "adj_close_data = adj_close_data.dropna(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "any(adj_close_data.isna().sum() > 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "returns_df = adj_close_data.pct_change().dropna().reset_index().rename(columns={'Date': 'date'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "returns_df_melt = returns_df.melt(id_vars='date', var_name='stock', value_name='daily_returns')\n",
        "returns_df_melt['date'] = pd.to_datetime(returns_df_melt['date'])\n",
        "returns_df_melt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "returns_df_melt.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "signals_df['date'] = pd.to_datetime(signals_df['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df = pd.merge(returns_df_melt, signals_df, on=['date', 'stock'], how='left').dropna()\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count number of stocks per date\n",
        "merged_df['num_stocks_by_date'] = merged_df.groupby('date').transform('size')\n",
        "\n",
        "# Select data where there were at least 10 stocks for each date\n",
        "merged_df_filtered = merged_df[merged_df['num_stocks_by_date'] >= 10]\n",
        "\n",
        "# Drop num_stocks_by_date column\n",
        "ml_df = merged_df_filtered.drop(columns='num_stocks_by_date').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Build Machine Learning Dataframes and Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Convert the 'date' column in both DataFrames to datetime format\n",
        "# ml_df['date'] = pd.to_datetime(ml_df['date'])\n",
        "# data_duplicate['date'] = pd.to_datetime(data_duplicate['date'])\n",
        "\n",
        "# # Merge ml_df with data_duplicate on the columns date and stock\n",
        "# data_additional_features = pd.merge(ml_df, data_duplicate[['date', 'stock', 'title', 'preprocessed_text', 'tokens']], on=['date', 'stock'], how='left')\n",
        "# data_additional_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Feature Engineer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Day of Week Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_df['date'] = pd.to_datetime(ml_df['date'])\n",
        "ml_df['day_of_week'] = ml_df['date'].dt.dayofweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Topic Feature using Latent Dirichlet Allocation (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_literal_eval(s):\n",
        "    try:\n",
        "        return ast.literal_eval(s)\n",
        "    except (ValueError, SyntaxError):\n",
        "        return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_df['tokens'] = ml_df['tokens'].apply(safe_literal_eval)\n",
        "\n",
        "# Create Dictionary (takes 5 minutes to run)\n",
        "id2word = corpora.Dictionary(ml_df['tokens'])\n",
        "\n",
        "# Term Document Frequency (Corpus)\n",
        "corpus = [id2word.doc2bow(text) for text in ml_df['tokens']]\n",
        "\n",
        "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=10, random_state=42, passes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect Topics\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
        "\n",
        "# Inspect Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_additional_features['tokens'], dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get the dominant topic\n",
        "def get_dominant_topic(lda_model, corpus):\n",
        "    dominant_topics = []\n",
        "    for doc_topics in lda_model[corpus]:\n",
        "        # Sort the topics by their assigned proportions\n",
        "        sorted_topics = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
        "        # Get the topic number of the highest proportion topic\n",
        "        dominant_topic = sorted_topics[0][0]\n",
        "        dominant_topics.append(dominant_topic)\n",
        "    return dominant_topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the dominant topic on the data\n",
        "ml_df['dominant_topic'] = get_dominant_topic(lda_model, corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Entities Count Feature using Named Entity Recognizer (NER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the spaCy model\n",
        "spacy_nlp_model = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_entities(text):\n",
        "    # Process the text with the NER model\n",
        "    doc = spacy_nlp_model(text)\n",
        "\n",
        "    # Extract entities that are either PERSON or ORG (companies). You can adjust this as needed.\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ in ['PERSON', 'ORG']]\n",
        "\n",
        "    return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract entities from data (takes 10 - 20 minutes to run)\n",
        "ml_df['entities'] = ml_df['title'].apply(extract_entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_df['entities_count'] = ml_df['entities'].apply(len)\n",
        "ml_df.drop('entities', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_df_final = ml_df[['date', 'stock', 'daily_returns', 'sentiment_score', 'day_of_week', 'dominant_topic', 'entities_count']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to pkl file\n",
        "save_pkl_file(ml_df_final, 'cache/dataframes/ml_df.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineer Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Functions for Train Test Split, One Hot Encoding, Scoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define function for train test split\n",
        "def ts_train_test_split(data, test_size):\n",
        "    \"\"\"Takes in data and output train set and test set in that order\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame or pd.Series): Data to split into train and test\n",
        "        test_size (float): Percentage for test size\n",
        "\n",
        "    Returns:\n",
        "        tuple: train set, test set\n",
        "    \"\"\"\n",
        "    train_size = 1-test_size\n",
        "    train_idx = round(X.shape[0] * train_size)\n",
        "    return data.iloc[:train_idx], data.iloc[train_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#define function for one hot encoding\n",
        "def one_hot_encode(X_train, X_valid):\n",
        "    # Initialize OneHotEncoder\n",
        "    one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "    \n",
        "        # Drop date column if it exists\n",
        "    X_train = X_train.drop(columns=['date'], errors='ignore')\n",
        "    X_valid = X_valid.drop(columns=['date'], errors='ignore')\n",
        "    \n",
        "    # One-hot Encode 'stock' Column for Training and Validation Data\n",
        "    OH_cols_train = pd.DataFrame(one_hot_encoder.fit_transform(X_train[['stock']]))\n",
        "    OH_cols_valid = pd.DataFrame(one_hot_encoder.transform(X_valid[['stock']]))\n",
        "\n",
        "    # Assign Column Names after One-Hot Encoding and Restore Index\n",
        "    OH_cols_train.columns = one_hot_encoder.get_feature_names_out(['stock'])\n",
        "    OH_cols_valid.columns = one_hot_encoder.get_feature_names_out(['stock'])\n",
        "    OH_cols_train.index, OH_cols_valid.index = X_train.index, X_valid.index\n",
        "\n",
        "    # Remove Original 'stock' Column\n",
        "    num_X_train = X_train.drop('stock', axis=1)\n",
        "    num_X_valid = X_valid.drop('stock', axis=1)\n",
        "\n",
        "    # Concatenate Original Data with One-Hot Encoded Columns\n",
        "    OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
        "    OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
        "\n",
        "    # Ensure all columns have string type\n",
        "    OH_X_train.columns = OH_X_train.columns.astype(str)\n",
        "    OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
        "    \n",
        "    return OH_X_train, OH_X_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score_model(model, X_t, X_v, y_t, y_v):\n",
        "    # Fit Model\n",
        "    model.fit(X_t, y_t)\n",
        "\n",
        "    # Predict\n",
        "    preds = model.predict(X_v)\n",
        "\n",
        "    # Check MAE\n",
        "    mae = mean_absolute_error(preds, y_v)\n",
        "    return preds, mae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Machine Learning Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split Data into Features and Target\n",
        "# Sort values by date\n",
        "X = ml_df.sort_values(\"date\").drop('daily_returns', axis=1).reset_index(drop=True)\n",
        "y = ml_df.sort_values(\"date\")['daily_returns'].reset_index(drop=True)\n",
        "\n",
        "# Split Data into Training and Validation Sets\n",
        "X_train, X_valid = ts_train_test_split(X, test_size=0.2)\n",
        "y_train, y_valid = ts_train_test_split(y, test_size=0.2)\n",
        "\n",
        "# Use one_hot_encode function to get One-Hot Encoded Training and Validation Data\n",
        "OH_X_train, OH_X_valid = one_hot_encode(X_train, X_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Fit linear model\n",
        "linear_model = LinearRegression()\n",
        "preds_linear, mae_linear = score_model(linear_model, OH_X_train, OH_X_valid, y_train, y_valid)\n",
        "print(f\"Mean Absolute Error with Linear Regression: {mae_linear}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Fit XGboost model\n",
        "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=5)\n",
        "preds_xgboost, mae_xgboost = score_model(xgb_model, OH_X_train, OH_X_valid, y_train, y_valid)\n",
        "print(f\"Mean Absolute Error with XGBoost: {mae_xgboost}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-Nearest Neighbours (KNN) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Fit K-Nearest Neighbours model\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=2000)\n",
        "preds_knn, mae_knn= score_model(knn_regressor, OH_X_train, OH_X_valid, y_train, y_valid)\n",
        "print(f\"Mean Absolute Error with KNN: {mae_knn}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cache KNN Model\n",
        "save_pkl_file(knn_regressor, filepath='cache/ml_models/knn_regressor.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in KNN Model\n",
        "knn_regressor = load_pkl_file('cache/ml_models/knn_regressor.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-layer Perceptron (MLP) Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the MLPRegressor model\n",
        "nn_model = MLPRegressor(hidden_layer_sizes=(128, 64, 32),\n",
        "                        activation='relu',\n",
        "                        solver='adam',\n",
        "                        max_iter=500,\n",
        "                        early_stopping=True, # To use early stopping based on validation score\n",
        "                        validation_fraction=0.1, # Fraction of training data to set aside as validation set for early stopping\n",
        "                        verbose=True,\n",
        "                        random_state=42)\n",
        "\n",
        "# Use the score_model function to fit and predict\n",
        "preds_nn, mae_nn = score_model(nn_model, OH_X_train, OH_X_valid, y_train, y_valid)\n",
        "\n",
        "# Print the MAE for the neural network model\n",
        "print(f\"Mean Absolute Error with Neural Network (MLPRegressor): {mae_nn}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cache KNN Model\n",
        "save_pkl_file(nn_model, filepath='cache/ml_models/nn_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in KNN Model\n",
        "nn_model = load_pkl_file('cache/ml_models/nn_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Support Vector Regression (SVR) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create SVR pipeline\n",
        "def create_svr_pipeline():\n",
        "    return make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.1, kernel='rbf'))\n",
        "\n",
        "# Use your score_model function\n",
        "pipeline = create_svr_pipeline()\n",
        "preds_svr, mae_svr = score_model(pipeline, OH_X_train, OH_X_valid, y_train, y_valid)\n",
        "\n",
        "# Print MAE\n",
        "print(f\"Mean Absolute Error with SVR: {mae_svr}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_pkl_file(pipeline, 'cache/ml_models/svr_pipeline.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = load_pkl_file('cache/ml_models/svr_pipeline.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Baseline Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the mean of the training target values\n",
        "mean_train = y_train.mean()\n",
        "\n",
        "# Use the mean to make predictions for the validation set\n",
        "mean_preds = [mean_train] * len(y_valid)\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE) of the mean model\n",
        "mae_mean = mean_absolute_error(y_valid, mean_preds)\n",
        "\n",
        "print(f\"Mean Absolute Error with Mean Model: {mae_mean}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arimax Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define external regressors. Ensure there are no non-numeric or NaN values.\n",
        "exog_train = OH_X_train\n",
        "exog_valid = OH_X_valid\n",
        "\n",
        "# Fit ARIMA-X model (ARIMA with external regressors, without the seasonal component) using the entire dataset\n",
        "model = ARIMA(y_train, exog=exog_train, order=(1,1,1))\n",
        "results = model.fit()\n",
        "\n",
        "# Forecast\n",
        "forecast = results.predict(start=len(y_train), end=len(y_train) + len(y_valid) - 1, exog=exog_valid, dynamic=True)\n",
        "\n",
        "# Calculate MAE\n",
        "mae_arimax = mean_absolute_error(y_valid, forecast)\n",
        "\n",
        "print(f\"Mean Absolute Error with ARIMA-X: {mae_arimax}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare with Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    'Linear': mae_linear,\n",
        "    'XGBoost': mae_xgboost,\n",
        "    'KNN': mae_knn,\n",
        "    'SVR' : mae_svr,\n",
        "    'NN' : mae_nn,\n",
        "    'Mean Model': mae_mean,\n",
        "    'Arima X Model': mae_arimax\n",
        "}\n",
        "\n",
        "# Sorting dictionary by MAE\n",
        "sorted_models = dict(sorted(models.items(), key=lambda item: item[1]))\n",
        "\n",
        "# Define colors for models\n",
        "colors = ['lightgrey' if model in ['Mean Model', 'Arima X Model'] else 'lightpink' for model in sorted_models.keys()]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12,6))\n",
        "bars = plt.barh(list(sorted_models.keys()), list(sorted_models.values()), color=colors)\n",
        "plt.xlabel('Mean Absolute Error (MAE)')\n",
        "plt.ylabel('Models')\n",
        "plt.title('Model Comparison based on MAE')\n",
        "\n",
        "# Annotate the bars with the actual MAE values\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_width() - 0.005, bar.get_y() + bar.get_height()/2, \n",
        "             f'{bar.get_width():.4f}', \n",
        "             va='center', ha='right', color='black', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Feature Determination based on Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Tuning Features from NLP Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Hyperparameter Tuning for Machine Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Portfolio Analytics Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_daily_ptf_rtn(ptf_wgt, returns_df, ls=False):\n",
        "    \"\"\"Calculate daily portfolio returns given long form portfolio weights and returns\n",
        "\n",
        "    Args:\n",
        "        ptf_wgt (pd.DataFrame): Long form id level portfolio weights\n",
        "        returns_df (pd.DataFrame): Long form id level returns\n",
        "    \"\"\"\n",
        "    ptf_wgt['DATE'] = pd.to_datetime(ptf_wgt['DATE'])\n",
        "    returns_df['DATE'] = pd.to_datetime(returns_df['DATE'])\n",
        "\n",
        "    start_date = ptf_wgt['DATE'].min()\n",
        "    end_date = ptf_wgt['DATE'].max()\n",
        "\n",
        "    filtered_returns = returns_df[(returns_df['DATE'] <= end_date) & (returns_df['DATE'] >= start_date)].reset_index(drop=True)\n",
        "    rebal_dates = ptf_wgt['DATE'].unique()\n",
        "\n",
        "    # Add rebal date column to returns df\n",
        "    filtered_returns['REBAL_DATE'] = filtered_returns['DATE'].apply(lambda x: rebal_dates[rebal_dates <= x].max())\n",
        "    joint_df = pd.merge(ptf_wgt.rename(columns={'DATE': 'REBAL_DATE'}), filtered_returns, on=['REBAL_DATE', 'ID'], how='left')\n",
        "\n",
        "    # Dates with no returns are filled as 0\n",
        "    joint_df = joint_df.fillna(0)\n",
        "\n",
        "    final_df = joint_df[['DATE', 'REBAL_DATE', 'ID', \"WGT\", \"RTN\"]].copy()\n",
        "\n",
        "    # Cumulate asset returns within each rebal date\n",
        "    final_df['ASSET_CUM_RTN'] = final_df.groupby(['REBAL_DATE', 'ID'])['RTN'].transform(lambda x: (1+x).cumprod())\n",
        "\n",
        "    # Calculate asset's MTM weight\n",
        "    final_df['MTM_WGT'] = final_df['WGT'] * final_df['ASSET_CUM_RTN']\n",
        "    final_df['DATE'] = pd.to_datetime(final_df['DATE'])\n",
        "\n",
        "    if ls:\n",
        "        final_df['LONG/SHORT'] = np.where(final_df['MTM_WGT'] > 0, 'LONG', 'SHORT')\n",
        "        final_df['PTF_MTM_BASE'] = final_df.groupby([\"DATE\", 'LONG/SHORT'])['MTM_WGT'].transform(lambda x: abs(x).sum())\n",
        "        final_df['ASSET_WEIGHTS'] = final_df['MTM_WGT'] / final_df['PTF_MTM_BASE']\n",
        "\n",
        "    else:\n",
        "        # Calculate portfolio MTM base weight\n",
        "        final_df['PTF_MTM_BASE'] = final_df.groupby(\"DATE\")['MTM_WGT'].transform('sum')\n",
        "\n",
        "        # Calculate renormed asset weights\n",
        "        final_df['ASSET_WEIGHTS'] = final_df['MTM_WGT'] / final_df['PTF_MTM_BASE']\n",
        "\n",
        "    # Shift asset weights down by 1 to represent implied lag of 1 day\n",
        "    final_df['ASSET_WEIGHTS_SHIFTED'] = final_df.groupby('ID')['ASSET_WEIGHTS'].shift(1)\n",
        "\n",
        "    # Drop NaNs introduced from shifting\n",
        "    final_df = final_df.dropna(axis=0)\n",
        "\n",
        "    ptf_rtn_df = final_df.groupby('DATE').apply(lambda x: (x['RTN'] * x['ASSET_WEIGHTS_SHIFTED']).sum()).reset_index(name=\"PTF_RTN\")\n",
        "    return ptf_rtn_df, final_df\n",
        "\n",
        "def calc_annualised_returns(cumulative_returns:float, n, frequency):\n",
        "    if frequency == \"D\":\n",
        "        t = 252\n",
        "    elif frequency == \"M\":\n",
        "        t = 12\n",
        "    return ((cumulative_returns + 1)**(t/n) - 1).values[0]\n",
        "\n",
        "def calc_annualised_vol(ptf_rtn: pd.Series, frequency):\n",
        "    if frequency == \"D\":\n",
        "        n = 252 # 252 trading days in ptf_rtn\n",
        "    elif frequency == \"M\":\n",
        "        n = 12\n",
        "    return ptf_rtn.std(ddof=1).values[0] * np.sqrt(n)\n",
        "\n",
        "def calc_max_dd(ptf_rtn: pd.Series):\n",
        "    # Cumulative returns must be base 1\n",
        "    ptf_cumulative_return = (1+ptf_rtn).cumprod()\n",
        "\n",
        "    # Calculate running max\n",
        "    running_max = ptf_cumulative_return.cummax()\n",
        "\n",
        "    # Drawdown\n",
        "    drawdown = (ptf_cumulative_return-running_max)/running_max\n",
        "\n",
        "    max_drawdown = drawdown.min().values[0]\n",
        "    return max_drawdown\n",
        "\n",
        "def calc_ptf_summary(ptf_rtn):\n",
        "    ptf_cum_rtn = (ptf_rtn+1).prod()-1\n",
        "    ptf_ann_rtn = calc_annualised_returns(ptf_cum_rtn, len(ptf_rtn), 'D')\n",
        "    ptf_ann_vol = calc_annualised_vol(ptf_rtn, \"D\")\n",
        "    ptf_max_dd = calc_max_dd(ptf_rtn)\n",
        "    sharpe_ratio = ptf_ann_rtn/ptf_ann_vol\n",
        "    downside_sd = ptf_rtn[ptf_rtn < 0].std()[0]\n",
        "    sortino_ratio = ptf_ann_rtn/downside_sd\n",
        "    return pd.DataFrame({\n",
        "        'Metrics': ['Cumulative Returns', 'Annualised Returns',\n",
        "                    'Annualised Volatility', 'Maximum Drawdown',\n",
        "                    'Sharpe Ratio', 'Sortino Ratio'],\n",
        "        'Values': [ptf_cum_rtn[0], ptf_ann_rtn, ptf_ann_vol, ptf_max_dd,\n",
        "                   sharpe_ratio, sortino_ratio]\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Backtest():\n",
        "    def __init__(self, fitted_model, rtn_df, OH_X_train, y_train, OH_X_valid, X_valid):\n",
        "        self.model = fitted_model\n",
        "        self.rtn_df = rtn_df\n",
        "        self.OH_X_train = OH_X_train\n",
        "        self.OH_X_valid = OH_X_valid\n",
        "        self.y_train = y_train\n",
        "        self.X_valid = X_valid\n",
        "\n",
        "        # Generate signal dataframe\n",
        "        self.sig_df =  self.gen_signals_df()\n",
        "\n",
        "        # Construct long short portfolio\n",
        "        self.ls_ptf_wgt = self.constr_ls_ptf_wgt()\n",
        "\n",
        "        # Construct long only portfolio\n",
        "        self.lo_ptf_wgt = self.constr_lo_ptf_wgt()\n",
        "\n",
        "        # Construct equal weighted portfolio\n",
        "        self.eq_ptf_wgt = self.constr_eq_ptf_wgt()\n",
        "\n",
        "        # Generate backtest analytics\n",
        "        self.ptf_rtn_combined, self.cum_rtn_fig, self.summary_metrics = self.gen_backtest_analytics()\n",
        "\n",
        "    def gen_signals_df(self):\n",
        "        preds = self.model.predict(self.OH_X_valid)\n",
        "        trading_df = self.X_valid.copy()\n",
        "\n",
        "        # Create column of predicted returns\n",
        "        trading_df['predicted_rtn'] = preds\n",
        "\n",
        "        # Count number of stocks per date\n",
        "        trading_df['num_stocks_by_date'] = trading_df.groupby('date').transform('size')\n",
        "\n",
        "        # Select data where there were at least 10 stocks for each date\n",
        "        trading_df_filtered = trading_df[trading_df['num_stocks_by_date'] >= 10]\n",
        "\n",
        "        trading_df_filtered = trading_df_filtered[['date', 'stock', 'predicted_rtn']].reset_index(drop=True)\n",
        "\n",
        "        # Handle duplicated stocks on a single date (stems from multiple headline for a stock on one day)\n",
        "        trading_df_filtered_dd = trading_df_filtered.groupby(['date', 'stock'])['predicted_rtn'].mean().reset_index(name='predicted_rtn')\n",
        "\n",
        "        # Create daily index and forward fill\n",
        "        start_date = trading_df_filtered_dd['date'].min()\n",
        "        end_date = trading_df_filtered_dd['date'].max()\n",
        "\n",
        "        # Reesample to daily\n",
        "        daily_index = pd.date_range(start=start_date, end=end_date, freq='B')\n",
        "\n",
        "        # Pivot dataframe to forward fill to daily index so that we have signals everyday\n",
        "        trading_df_filtered_wide = trading_df_filtered_dd.pivot(index='date', values='predicted_rtn', columns='stock')\n",
        "        trading_df_filtered_wide = trading_df_filtered_wide.reindex(daily_index).fillna(method='ffill')\n",
        "\n",
        "        final_signal_df = (trading_df_filtered_wide\n",
        "                        .reset_index(names='date')\n",
        "                        .rename_axis(None, axis=1)\n",
        "                        .melt(id_vars='date', var_name='stock', value_name='signal')\n",
        "                        .dropna(axis=0).reset_index(drop=True))\n",
        "        final_signal_df = final_signal_df.rename(columns={\n",
        "            'date': 'DATE',\n",
        "            'stock': 'ID',\n",
        "            'signal': 'SIGNAL'\n",
        "        })\n",
        "        return final_signal_df\n",
        "\n",
        "    def constr_ls_ptf_wgt(self):\n",
        "        # Create copy of sig_df\n",
        "        sig_df = self.sig_df.copy()\n",
        "\n",
        "        # Ranking the signals to reduce fat tails\n",
        "        sig_df['RANKED_SIGNAL'] = sig_df.groupby('DATE')['SIGNAL'].transform(lambda x: scipy.stats.rankdata(x))\n",
        "        sig_df = sig_df.sort_values(['DATE', 'ID']).reset_index(drop=True)\n",
        "\n",
        "        # Long short weights calculated as the distance for median signal for each date\n",
        "        sig_df['WGT'] = sig_df.groupby(['DATE'])['RANKED_SIGNAL'].transform(lambda x: x-x.median())\n",
        "\n",
        "        # Renormalise weights to $1 long $1 short – dollar neutral strategy\n",
        "        sig_df['DIRECTION'] = np.where(sig_df['WGT']>=0, 'LONG', 'SHORT')\n",
        "        sig_df['RENORM_WGT'] = sig_df.groupby(['DATE', 'DIRECTION'])['WGT'].transform(lambda x: x/np.abs(x.sum()))\n",
        "        return sig_df[['DATE', 'ID', 'RENORM_WGT']].rename(columns={'RENORM_WGT': 'WGT'})\n",
        "\n",
        "    def constr_lo_ptf_wgt(self):\n",
        "        # Create copy of sig_df\n",
        "        sig_df = self.sig_df.copy()\n",
        "\n",
        "        # Create long only signal df\n",
        "        sig_df_lo = sig_df[sig_df['SIGNAL']>0].copy()\n",
        "\n",
        "        # Reset index\n",
        "        sig_df_lo = sig_df_lo.reset_index(drop=True)\n",
        "\n",
        "        # Use predicted returns as weights\n",
        "        sig_df_lo['WGT'] = sig_df_lo.groupby('DATE')['SIGNAL'].transform(lambda x: x/x.sum())\n",
        "        return sig_df_lo[['DATE', 'ID', 'WGT']]\n",
        "\n",
        "    def constr_eq_ptf_wgt(self):\n",
        "        # Create copy of sig_df\n",
        "        sig_df = self.sig_df.copy()\n",
        "\n",
        "        # Use predicted returns as weights\n",
        "        sig_df['WGT'] = sig_df.groupby('DATE')['ID'].transform(lambda x: 1/x.shape[0])\n",
        "        return sig_df[['DATE', 'ID', 'WGT']]\n",
        "\n",
        "    def calc_ptf_rtn(self, ptf_wgt):\n",
        "        # Merge ls weights on returns – left join\n",
        "        ptf_df = pd.merge(ptf_wgt, self.rtn_df, on=['DATE', 'ID'], how='left')\n",
        "\n",
        "        # On dates without returns just set returns to 0\n",
        "        ptf_df = ptf_df.fillna(0)\n",
        "\n",
        "        # Shift weights by 1 to imply lag\n",
        "        ptf_df['WGT_SHIFTED'] = ptf_df['WGT'].shift(1)\n",
        "\n",
        "        # Calculate long short weighted returns\n",
        "        ptf_df['WGT_RTN'] = ptf_df['WGT_SHIFTED'] * ptf_df['RTN']\n",
        "\n",
        "        # Drop NaN values from shifting\n",
        "        ptf_df = ptf_df.dropna()\n",
        "\n",
        "        ptf_rtn = ptf_df.groupby('DATE')['WGT_RTN'].sum().reset_index(name='PTF_RTN')\n",
        "        return ptf_rtn\n",
        "\n",
        "    def gen_backtest_analytics(self):\n",
        "        ls_ptf_rtn = self.calc_ptf_rtn(self.ls_ptf_wgt)\n",
        "        ls_ptf_rtn['PORT'] = \"Long Short Portfolio\"\n",
        "\n",
        "        lo_ptf_rtn = self.calc_ptf_rtn(self.lo_ptf_wgt)\n",
        "        lo_ptf_rtn['PORT'] = \"Long Only Portfolio\"\n",
        "\n",
        "        eq_ptf_rtn = self.calc_ptf_rtn(self.eq_ptf_wgt)\n",
        "        eq_ptf_rtn['PORT'] = \"Equal Weighted Portfolio\"\n",
        "\n",
        "        ptf_rtn_combined = pd.concat([ls_ptf_rtn, lo_ptf_rtn, eq_ptf_rtn], axis=0)\n",
        "\n",
        "        # Cumulate portfolio returns\n",
        "        ptf_rtn_combined['CUM_RTN'] = ptf_rtn_combined.groupby(['PORT'])['PTF_RTN'].transform(lambda x: (1+x).cumprod())\n",
        "\n",
        "        # Generate cumulative returns figure\n",
        "        fig = px.line(ptf_rtn_combined, x='DATE', y='CUM_RTN', color='PORT')\n",
        "        fig.update_layout(hovermode='x unified',\n",
        "                          title=\"Portfolio Performance <br><sup></sup>\",\n",
        "                          yaxis_title='Cumulative Returns')\n",
        "        fig.add_hline(y=1, line_dash='dash')\n",
        "\n",
        "        # Create summary metrics table\n",
        "        summary_metrics = (linear_model_backtest.ptf_rtn_combined.groupby('PORT')\n",
        "                            .apply(lambda x: calc_ptf_summary(x.set_index('DATE')))\n",
        "                            .reset_index(level=0)\n",
        "                            .pivot(index='Metrics', columns='PORT', values='Values')\n",
        "                            .reset_index().rename_axis(None, axis=1)\n",
        "                            )\n",
        "        summary_metrics.columns = ['Metrics', 'Equal Weighted Portfolio', 'Signals-Weighted Long Only Portfolio', 'Signals-Weighted Long Short Portfolio']\n",
        "        return ptf_rtn_combined, fig, summary_metrics\n",
        "\n",
        "    def display_results(self):\n",
        "        display(self.summary_metrics)\n",
        "        self.cum_rtn_fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Long Short Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rtn_df = returns_df_melt.rename(columns={'date': 'DATE', 'stock': 'ID', 'daily_returns': 'RTN'})\n",
        "rtn_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linear Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linear_model_backtest = Backtest(linear_model, rtn_df, OH_X_train, y_train, OH_X_valid, X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linear_model_backtest.display_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KNN Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn_model = load_pkl_file('cache/ml_models/nn_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn_model_backtest = Backtest(nn_model, rtn_df, OH_X_train, y_train, OH_X_valid, X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn_model_backtest.display_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Long only Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample\n",
        "sig_df = nn_model_backtest.sig_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use SIGNAL (i.e. predicted returns) as weights\n",
        "# Create long only signal df\n",
        "sig_df_lo = sig_df[sig_df['SIGNAL']>0].copy()\n",
        "\n",
        "sig_df_lo = sig_df_lo.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use predicted returns as weights\n",
        "sig_df_lo['WGT'] = sig_df_lo.groupby('DATE')['SIGNAL'].transform(lambda x: x/x.sum())\n",
        "sig_df_lo\n",
        "sig_df_lo[['DATE', 'ID', 'WGT']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MPT Optimised Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating all ML Models Trading Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model in os.listdir('cache/ml_models/'):\n",
        "    ml_model = load_pkl_file(f'cache/ml_models/{model}')\n",
        "    backtester = Backtest(ml_model, rtn_df, OH_X_train, y_train, OH_X_valid, X_valid)\n",
        "    backtester.display_results()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
